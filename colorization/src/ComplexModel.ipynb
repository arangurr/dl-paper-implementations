{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ComplexModel.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"7Agqbsok67VJ","colab_type":"text"},"cell_type":"markdown","source":["# Image Colorization using CNNs\n","\n","In this notebook we explore how to produce a plausible colorized image from a grayscale image."]},{"metadata":{"id":"tANVOAg47lWZ","colab_type":"text"},"cell_type":"markdown","source":["## Setup"]},{"metadata":{"id":"nEpD1Q-n7TG6","colab_type":"text"},"cell_type":"markdown","source":["We are going to use Google Drive's storage to save and load data. First, we need to authenticate our user.\n","We assume everything will be inside a folder named 'shared' in the root of Drive. This cell can be skipped, but then the model won't be saved later using our method."]},{"metadata":{"id":"rG06ooKy2ud7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"ad062894-26f8-4b86-bb80-eeb7ffe82bd1","executionInfo":{"status":"ok","timestamp":1542690687346,"user_tz":360,"elapsed":5201,"user":{"displayName":"Jaime Cernuda Garcia","photoUrl":"","userId":"05776885657832907950"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"9u-aqSSt7s8W","colab_type":"text"},"cell_type":"markdown","source":["Other imports that will be used throughout the code:"]},{"metadata":{"id":"_NaAqSsKKD0C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"28f6d311-05af-4b1c-a13b-9d1d93d1bb08","executionInfo":{"status":"ok","timestamp":1542690692330,"user_tz":360,"elapsed":6526,"user":{"displayName":"Jaime Cernuda Garcia","photoUrl":"","userId":"05776885657832907950"}}},"cell_type":"code","source":["from keras.datasets import cifar10\n","!pip install tqdm # for progress bar support\n","!pip install parmap\n","import parmap\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","import sklearn.neighbors as nn\n","from scipy.interpolate import interp1d\n","from scipy.signal import gaussian, convolve\n","from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n","import os\n","\n","import keras\n","from keras.optimizers import Adam\n","from keras import backend as K\n","from keras.models import Sequential\n","from keras.layers import Conv2D, BatchNormalization, ZeroPadding2D, Conv2DTranspose, Lambda, Softmax, Add, Input, Activation\n","from keras.preprocessing.image import ImageDataGenerator"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n","Requirement already satisfied: parmap in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"],"name":"stdout"}]},{"metadata":{"id":"SZZK4ZSdfjj2","colab_type":"text"},"cell_type":"markdown","source":["## Model definition\n","\n","First we define the model hyperparameters as follows:\n"]},{"metadata":{"id":"FIYOjbyneqoO","colab_type":"code","colab":{}},"cell_type":"code","source":["h = 32\n","w = h\n","input_shape = (h,w,1) # Channels last\n","batch_size = 1\n","epochs = 5\n","nb_classes = 313"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lTEFmuDugN9g","colab_type":"text"},"cell_type":"markdown","source":["We need to import the file pts_in_hull.npy so we define it's location"]},{"metadata":{"id":"rTiacA5R8oD8","colab_type":"code","colab":{}},"cell_type":"code","source":["(raw_train, _), (raw_test, _) = cifar10.load_data()\n","\n","data_dir = \"./drive/My Drive/shared/\"\n","q_ab = np.load(os.path.join(data_dir, \"pts_in_hull.npy\"))\n","nb_q = q_ab.shape[0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d-DbGuNnXzTR","colab_type":"text"},"cell_type":"markdown","source":["Then we define some of the required supporting functions"]},{"metadata":{"id":"lbfK3-2hcIBh","colab_type":"code","colab":{}},"cell_type":"code","source":["def calcualte_factor(raw_train, q_ab, nearest):\n","  gamma=0.5\n","  alpha=1\n","  sigma=5\n","  n, h, w, c = raw_train.shape\n","  \n","  X_a = np.ravel(raw_train[:, :, :, 0])\n","  X_b = np.ravel(raw_train[:, :, :, 1])\n","  X_ab = np.vstack((X_a, X_b)).T\n","\n","  # Find index of nearest neighbor for X_ab\n","  _, ind = nearest.kneighbors(X_ab)\n","\n","  # We now count the number of occurrences of each color\n","  ind = np.ravel(ind)\n","  counts = np.bincount(ind)\n","  idxs = np.nonzero(counts)[0]\n","  prior_prob = np.zeros((q_ab.shape[0]))\n","  for i in range(q_ab.shape[0]):\n","      prior_prob[idxs] = counts[idxs]\n","\n","  # We turn this into a color probability\n","  prior_prob = prior_prob / (1.0 * np.sum(prior_prob))\n","\n","  # add an epsilon to prior prob to avoid 0 vakues and possible NaN\n","  prior_prob += 1E-3 * np.min(prior_prob)\n","  # renormalize\n","  prior_prob = prior_prob / (1.0 * np.sum(prior_prob))\n","\n","  # Smooth with gaussian\n","  f = interp1d(np.arange(prior_prob.shape[0]),prior_prob)\n","  xx = np.linspace(0,prior_prob.shape[0] - 1, 1000)\n","  yy = f(xx)\n","  window = gaussian(2000, sigma)  # 2000 pts in the window, sigma=5\n","  smoothed = convolve(yy, window / window.sum(), mode='same')\n","  fout = interp1d(xx,smoothed)\n","  prior_prob_smoothed = np.array([fout(i) for i in range(prior_prob.shape[0])])\n","  prior_prob_smoothed = prior_prob_smoothed / np.sum(prior_prob_smoothed)\n","\n","  u = np.ones_like(prior_prob_smoothed)\n","  u = u / np.sum(1.0 * u)\n","\n","  prior_factor = (1 - gamma) * prior_prob_smoothed + gamma * u\n","  prior_factor = np.power(prior_factor, -alpha)\n","\n","  # renormalize\n","  prior_factor = prior_factor / (np.sum(prior_factor * prior_prob_smoothed))\n","\n","  return prior_factor"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MFjIsVbPenRu","colab_type":"code","colab":{}},"cell_type":"code","source":["def transformY(y, nearest, prior_factor, nb_q): \n","  Y_a = np.ravel(y[ :, :, 0])\n","  Y_b = np.ravel(y[ :, :, 1])\n","  Y_batch_ab = np.vstack((Y_a, Y_b)).T\n","  s = Y_batch_ab.shape[0]\n","  \n","  #Calculate nearest color cell to the pixels ab\n","  _, idx_neigh = nearest.kneighbors(Y_batch_ab)\n","  \n","  del Y_batch_ab\n","  \n","  #Y has size numpixels * numimages, 313 (number of color cells)\n","  Y = np.empty((s, nb_q))\n","  #idx_pts = np.arange(s)[:, np.newaxis]\n","  \n","  #We load Y with the color cell of every pixel\n","  Y[np.arange(s)[:, np.newaxis], idx_neigh] = np.ones((len(idx_neigh), 1))\n","  \n","  #find the actual cell color from the 313\n","  idx_max = np.argmax(Y, axis=1)\n","  \n","  #Add the color correction of the corresponding cell\n","  weights = prior_factor[idx_max].reshape(Y.shape[0], 1)\n","  #Append the weigts. 313 ->314\n","  Y = np.concatenate((Y, weights), axis=1)\n","  #Reshape into normal image\n","  h, w, c = y.shape\n","  Y = Y.reshape((h, w, nb_q + 1))\n","\n","  return Y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ECB1WTXOh4I1","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"From built-in optimizer classes.\n","\"\"\"\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import six\n","import copy\n","from six.moves import zip\n","\n","from keras import backend as K\n","from keras.utils.generic_utils import serialize_keras_object\n","from keras.utils.generic_utils import deserialize_keras_object\n","from keras.legacy import interfaces\n","\n","from keras.optimizers import Optimizer\n","\n","class AdamW(Optimizer):\n","    \"\"\"AdamW optimizer.\n","    Default parameters follow those provided in the original paper.\n","    # Arguments\n","        lr: float >= 0. Learning rate.\n","        beta_1: float, 0 < beta < 1. Generally close to 1.\n","        beta_2: float, 0 < beta < 1. Generally close to 1.\n","        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n","        decay: float >= 0. Learning rate decay over each update.\n","        weight_decay: float >= 0. Weight decay (L2 penalty) (default: 0.025).\n","        batch_size: integer >= 1. Batch size used during training.\n","        samples_per_epoch: integer >= 1. Number of samples (training points) per epoch.\n","        epochs: integer >= 1. Total number of epochs for training. \n","    # References\n","        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n","        - [Fixing Weight Decay Regularization in Adam](https://arxiv.org/abs/1711.05101)\n","    \"\"\"\n","\n","    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n","                 epsilon=None, decay=0., weight_decay=0.025, \n","                 batch_size=1, samples_per_epoch=1, \n","                 epochs=1, **kwargs):\n","        super(AdamW, self).__init__(**kwargs)\n","        with K.name_scope(self.__class__.__name__):\n","            self.iterations = K.variable(0, dtype='int64', name='iterations')\n","            self.lr = K.variable(lr, name='lr')\n","            self.beta_1 = K.variable(beta_1, name='beta_1')\n","            self.beta_2 = K.variable(beta_2, name='beta_2')\n","            self.decay = K.variable(decay, name='decay')\n","            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n","            self.batch_size = K.variable(batch_size, name='batch_size')\n","            self.samples_per_epoch = K.variable(samples_per_epoch, name='samples_per_epoch')\n","            self.epochs = K.variable(epochs, name='epochs')\n","        if epsilon is None:\n","            epsilon = K.epsilon()\n","        self.epsilon = epsilon\n","        self.initial_decay = decay\n","\n","    @interfaces.legacy_get_updates_support\n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        self.updates = [K.update_add(self.iterations, 1)]\n","\n","        lr = self.lr\n","        if self.initial_decay > 0:\n","            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n","                                                      K.dtype(self.decay))))\n","\n","        t = K.cast(self.iterations, K.floatx()) + 1\n","        '''Bias corrections according to the Adam paper\n","        '''\n","        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n","                     (1. - K.pow(self.beta_1, t)))\n","\n","        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","        self.weights = [self.iterations] + ms + vs\n","\n","        for p, g, m, v in zip(params, grads, ms, vs):\n","            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n","            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n","            \n","            '''Schedule multiplier eta_t = 1 for simple AdamW\n","            According to the AdamW paper, eta_t can be fixed, decay, or \n","            also be used for warm restarts (AdamWR to come). \n","            '''\n","            eta_t = 1.\n","            p_t = p - eta_t*(lr_t * m_t / (K.sqrt(v_t) + self.epsilon))\n","            if self.weight_decay != 0:\n","                '''Normalized weight decay according to the AdamW paper\n","                '''\n","                w_d = self.weight_decay*K.sqrt(self.batch_size/(self.samples_per_epoch*self.epochs))\n","                p_t = p_t - eta_t*(w_d*p) \n","\n","            self.updates.append(K.update(m, m_t))\n","            self.updates.append(K.update(v, v_t))\n","            new_p = p_t\n","\n","            # Apply constraints.\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(K.update(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {'lr': float(K.get_value(self.lr)),\n","                  'beta_1': float(K.get_value(self.beta_1)),\n","                  'beta_2': float(K.get_value(self.beta_2)),\n","                  'decay': float(K.get_value(self.decay)),\n","                  'weight_decay': float(K.get_value(self.weight_decay)),\n","                  'batch_size': int(K.get_value(self.batch_size)),\n","                  'samples_per_epoch': int(K.get_value(self.samples_per_epoch)),\n","                  'epochs': int(K.get_value(self.epochs)),\n","                  'epsilon': self.epsilon}\n","        base_config = super(AdamW, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zCm_yHOfX3Y1","colab_type":"code","colab":{}},"cell_type":"code","source":["def categorical_crossentropy_color(y_true, y_pred):\n","  # Flatten\n","  y_pred = K.reshape(y_pred, (batch_size * h * w, nb_classes+1))\n","  y_true = K.reshape(y_true, (batch_size * h * w, nb_classes+1))\n","\n","  weights = y_true[:, nb_classes:]  # extract weight from y_true\n","  weights = K.concatenate([weights] * nb_classes, axis=1)\n","  y_true = y_true[:, :-1]  # remove last column\n","  y_pred = y_pred[:, :-1]  # remove last column\n","  # multiply y_true by weights\n","  y_true = y_true * weights\n","\n","  cross_ent = K.categorical_crossentropy(y_pred, y_true)\n","  cross_ent = K.mean(cross_ent, axis=-1)\n","  return cross_ent"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M7bA1s--fB8a","colab_type":"text"},"cell_type":"markdown","source":["We define the model"]},{"metadata":{"id":"D8EIa14qX5vC","colab_type":"code","colab":{}},"cell_type":"code","source":["model = Sequential()\n","\n","model.add(Conv2D(64, kernel_size=3, name='conv1', input_shape=input_shape, activation='relu', padding='same'))\n","\n","for i in range(0,3):\n","  model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n","  model.add(BatchNormalization(axis=-1))\n","  model.add(Activation(\"relu\"))\n","  \n","  model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n","  model.add(BatchNormalization(axis=-1))\n","  model.add(Activation(\"relu\"))\n","\n","model.add(Conv2D(nb_classes, (1, 1), name=\"convFinal\", padding=\"same\"))\n","\n","# Reshape Softmax\n","def output_shape(input_shape):\n","    return (batch_size, h, w, nb_classes + 1)\n","\n","def reshape_softmax(x):\n","#     x = K.permute_dimensions(x, [0, 2, 3, 1])  # last dimension in number of filters\n","    x = K.reshape(x, (batch_size * h * w, nb_classes))\n","    x = K.softmax(x)\n","    # Add a zero column so that x has the same dimension as the target (313 classes + 1 weight)\n","    xc = K.zeros((batch_size * h * w, 1))\n","    x = K.concatenate([x, xc], axis=1)\n","    # Reshape back to (batch_size, h, w, nb_classes + 1) to satisfy keras' shape checks\n","    x = K.reshape(x, (batch_size, h, w, nb_classes + 1))\n","    return x\n","\n","model.add(Lambda(lambda z: reshape_softmax(z), name=\"ReshapeSoftmax\"))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DDubxGaDQqAT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":836},"outputId":"2e5fcfe5-afc8-4f80-f3ff-9015c49240a7","executionInfo":{"status":"ok","timestamp":1542690723071,"user_tz":360,"elapsed":825,"user":{"displayName":"Jaime Cernuda Garcia","photoUrl":"","userId":"05776885657832907950"}}},"cell_type":"code","source":["model.summary()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1 (Conv2D)               (None, 32, 32, 64)        640       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 32, 32, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 32, 32, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 32, 32, 64)        256       \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 32, 32, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 32, 32, 64)        256       \n","_________________________________________________________________\n","activation_6 (Activation)    (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","convFinal (Conv2D)           (None, 32, 32, 313)       20345     \n","_________________________________________________________________\n","ReshapeSoftmax (Lambda)      (1, 32, 32, 314)          0         \n","=================================================================\n","Total params: 244,089\n","Trainable params: 243,321\n","Non-trainable params: 768\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"kATyD0e__a55","colab_type":"code","colab":{}},"cell_type":"code","source":["adamw = AdamW(lr=10e-5, beta_1=0.9, beta_2=0.999, weight_decay=1e-3, batch_size=batch_size, epochs=epochs)\n","model.compile(loss=categorical_crossentropy_color, optimizer=adamw)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"geKrJRN89AK2","colab_type":"text"},"cell_type":"markdown","source":["## Model training\n","\n","Before actually training the model, we have to specify how we are going to feed the data. We will use a Generator derived from the Sequence class so we can use multiprocessing"]},{"metadata":{"id":"T5PUatCm5fnT","colab_type":"code","colab":{}},"cell_type":"code","source":["class DataGenerator(keras.utils.Sequence):\n","  \n","  def __init__(self, data, batch_size=16, dim=(32,32), shuffle=True):\n","    'Initialization'\n","    self.dim = dim\n","    self.data = data\n","    self.batch_size = batch_size\n","    self.shuffle = shuffle\n","    self.on_epoch_end()\n","    print(\"generator initialzed with, \", len(data))\n","    \n","  def on_epoch_end(self):\n","    self.indexes = np.arange(len(self.data))\n","    if self.shuffle == True:\n","      np.random.shuffle(self.indexes)\n","      \n","  def __data_generation(self, idx_temp):\n","    \n","    X = np.empty((self.batch_size, *self.dim, 1)) # 1 Channel\n","    y = np.empty((self.batch_size, *self.dim, 314)) # 313+1 channels\n","    \n","    for i, idx in enumerate(idx_temp):\n","      lab = np.asarray(rgb2lab(self.data[idx]))\n","      \n","      X[i,] = lab[:,:,:1]/100.\n","      y[i,] = transformY(lab[:,:,1:3], nearest, prior_factor, nb_q)\n","    \n","    return X, y\n","  \n","  def __len__(self):\n","    'Denotes the number of batches per epoch'\n","    return int(np.floor(len(self.data) / self.batch_size))\n","  \n","  def __getitem__(self, index):\n","    'Generate one batch of data'\n","    # Generate indexes of the batch\n","    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","    # Find list of IDs\n","    list_IDs_temp = [self.indexes[k] for k in indexes]\n","\n","    # Generate data\n","    X, y = self.__data_generation(list_IDs_temp)\n","\n","    return X, y\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_q2OQSb_9h-X","colab_type":"text"},"cell_type":"markdown","source":["Actual training using the CIFAR10 dataset using a train and validation set."]},{"metadata":{"id":"tQ8u6QszERgf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1504},"outputId":"fde8234f-6c7e-4380-f5ba-ea3b9d366ec9","executionInfo":{"status":"error","timestamp":1542690894618,"user_tz":360,"elapsed":134093,"user":{"displayName":"Jaime Cernuda Garcia","photoUrl":"","userId":"05776885657832907950"}}},"cell_type":"code","source":["# Create nearest neighbord instance with index = q_ab\n","nearest = nn.NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(q_ab)\n","prior_factor = calcualte_factor(raw_train, q_ab, nearest)\n","\n","training_generator = DataGenerator(data=raw_train, batch_size=batch_size, shuffle=False)\n","testing_generator = DataGenerator(data=raw_test,  batch_size=batch_size, shuffle=False)\n","\n","import multiprocessing\n","\n","workers = multiprocessing.cpu_count()\n","model.fit_generator(training_generator,\n","                    validation_data=testing_generator,\n","                    epochs=epochs,\n","                    use_multiprocessing=True,\n","                    workers=workers,\n","                    verbose=1)\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["generator initialzed with,  50000\n","generator initialzed with,  10000\n","Epoch 1/5\n"," 2726/50000 [>.............................] - ETA: 27:51 - loss: 6.4047"],"name":"stdout"},{"output_type":"stream","text":["Process ForkPoolWorker-4:\n","Process ForkPoolWorker-3:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","Process ForkPoolWorker-1:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n","    res = self._reader.recv_bytes()\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n","    buf = self._recv_bytes(maxlength)\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n","    with self._rlock:\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n","    buf = self._recv(4)\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-41ad5513b5c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     verbose=1)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 array_vals.append(\n\u001b[1;32m   2654\u001b[0m                     np.asarray(value,\n\u001b[0;32m-> 2655\u001b[0;31m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"hOxd1-8I-1da","colab_type":"text"},"cell_type":"markdown","source":["Save the model to Google Drive. "]},{"metadata":{"id":"aiwVk7yKpCTF","colab_type":"code","colab":{}},"cell_type":"code","source":["model.save(\"./drive/My Drive/shared/model_b{}_e{}.h5\".format(batch_size, epochs))\n","model.save_weights(\"./drive/My Drive/shared/model_b{}_e{}_weights.h5\".format(batch_size, epochs))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b1RTiSAkbJum","colab_type":"text"},"cell_type":"markdown","source":["Alternatively, we can save the model to the current working environment using:"]},{"metadata":{"id":"5IdpSynObQkI","colab_type":"code","colab":{}},"cell_type":"code","source":["model.save(\"model.h5\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"87LqLncv-8da","colab_type":"text"},"cell_type":"markdown","source":["## Model validation"]},{"metadata":{"id":"g5v7gFPEf0br","colab_type":"text"},"cell_type":"markdown","source":["To perform validation , and due to some  of the hardcoded parameters of our model, it is necesary to run the Colorize_Complex.py script"]}]}